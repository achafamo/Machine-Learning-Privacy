import os
import paramiko
import pickle
import re
import sys
import time

# todo: make it possible to execute batch and then get results later...
# todo: make files unique to job so multiple jobs can be submit at once
# todo: cleanup where log files go...

# CONFIGURATION

SOURCE = '/Users/mhay/Dropbox/research/pclass/'
REMOTE = 'mhay@biomath.colgate.edu'
REMOTE_IP = '149.43.143.10'
DEST = '/home/mhay/expt_home/'  # this folder should be empty!  stuff will be copied over to it
JOBNAME = 'pclass'
INFILENAME = 'payload.pkl'
OUTFILENAME = 'out.{jobid}.pkl'

class PBS(object):

    def __init__(self, batch):

        self.batch = batch
        self.numjobs = len(batch)

        # pickle data
        pickle.dump(batch, open(self._src_path(INFILENAME), 'w'))

        # write qsub sh script
        outf = open(self._src_path('execute_pickled.sh'), 'w')
        outf.write(SCRIPT_TEMPLATE.format(jobname=JOBNAME,
            infilename=INFILENAME,
            dest=DEST,
            outfilename=OUTFILENAME.format(jobid='$PBS_ARRAYID'),
            numjobs=self.numjobs-1))
        outf.close()

        self.ssh = paramiko.SSHClient()
        self.ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        self.ssh.connect(REMOTE_IP)

        # check that source and dest directories exist
        assert os.path.exists(SOURCE)
        cmd = "[ ! -d {0} ] && echo 'Directory not found'".format(DEST)
        stdout = self._exec_command(cmd)
        print stdout
        assert 'Directory not found' not in stdout

        # todo: less hacky way of doing this?
        cmd = 'rsync -vazP --exclude-from={src}/harness/excluded.txt {src} {remote}:{dest}'.format(src=SOURCE, remote=REMOTE, dest=DEST)
        os.system(cmd)

        # todo: less hacky way of doing this?
        self._remove_output_files()

    def _remove_file(self, filename):
        return self._exec_command('rm -f {0}'.format(self._dest_path(filename)))

    def _exec_command(self, cmd):
        stdin, stdout, stderr = self.ssh.exec_command(cmd)
        stdout = ''.join(stdout.readlines()).strip()
        stderr = ''.join(stderr.readlines()).strip()
        print >>sys.stderr, stderr
        return stdout

    def _remove_output_files(self):
        # remove old job files, if they exist
        for jobid in range(self.numjobs):
            outfilename = OUTFILENAME.format(jobid=jobid)
            self._remove_file(outfilename)

    def _src_path(self, filename):
        return os.path.join(SOURCE, filename)

    def _dest_path(self, filename):
        return DEST + filename

    def qsub(self):
        stdout = self._exec_command("qsub {0}".format(self._dest_path('execute_pickled.sh')))
        self.jobid = re.match(r'(\d+)', stdout).group(1)
        print "SUBMITTED", self.jobid, "which has", self.numjobs, "jobs"

    def unfinished(self):
        stdout = self._exec_command("qstat -t")
        print stdout
        lines = stdout.split('\n')[2:]  # first two lines are header information
        myjobs = [line for line in lines if self.jobid in line]
        print "UNFINISHED JOBS", len(myjobs)
        return len(myjobs)

    def retrieve(self):
        results = []
        ftp = self.ssh.open_sftp()
        for jobid in range(self.numjobs):
            outfilename = OUTFILENAME.format(jobid=jobid)
            stdout = self._exec_command('ls {0}'.format(self._dest_path(outfilename)))
            print stdout.strip()
            if outfilename in stdout:
                ftp.get(self._dest_path(outfilename), self._src_path(outfilename))
                result = pickle.load(open(self._src_path(outfilename)))
                os.remove(self._src_path(outfilename))  # remove copied file after data been obtained 
                results.append(result)
            else:
                print >>sys.stderr, 'COULD NOT FIND FILE:', outfilename
        ftp.close()
        return results

    def cleanup(self):
        self._remove_file(INFILENAME)
        self._remove_output_files()



#todo: undo hacks below to find correct python on biomath

#PBS -S /bin/bash

SCRIPT_TEMPLATE = '''
#
# ----------------------------------------------------------
# Note: this file was automatically generated by pbs_util.py
# ----------------------------------------------------------
#
# DEFAULT SETTINGS: not necessary to change
#PBS -S /bin/tcsh
#PBS -l nodes=1:ppn=1
#PBS -l mem=1GB
#PBS -q biomath
#PBS -j oe         
#PBS -l walltime=24:00:00 
#PBS -r y

# EXPERIMENT SPECIFIC SETTINGS: change thesecat

#PBS -N {jobname}          # name of job
#PBS -t 0-{numjobs}        # submit job array

cd {dest}          # change to directory from which job was submitted

# EXPERIMENT ITSELF: command to run experiment
PYTHONPATH=${{PYTHONPATH}}:/home/mhay/expt_home:/home/mhay/expt_home/harness:
LD_LIBRARY_PATH=${{LD_LIBRARY_PATH}}:/usr/lib64/atlas:/usr/local/Dist/python-2.7.6/lib
echo "PYTHONPATH IS" $PYTHONPATH
echo "which python" `which python`
echo $SHELL
python --version
date
/usr/local/bin/python2.7 harness/execute_pickled.py {infilename} {outfilename} $PBS_ARRAYID
'''

